# B1: Analyzing Data and Drawing Robust Conclusions

## 80/20 Assumption Audit

Do not audit all inputs equally. Map assumptions → rank by impact → focus on the few that drive most volume/value.

**Steps:**
1. Stop at the "analysis layer" before accepting any plan as complete — polished plans can still be wrong underneath
2. Map all assumptions, then rank by impact (80/20)
3. For the top drivers, pressure-test credibility:
   - If data is from a **third party** → ask: "What's their vested interest?"
   - If a number is a **plug** (used to make the total "work") → treat it as high-risk
4. Reframe the problem statement if the entire forecast may be unreliable

**Warning sign:** When a plan "adds up perfectly," suspect a plug. Ask: "Which line item was backfilled to hit the target?" Third-party numbers are not facts — they are often sales narratives.

---

## Definition Drill (Metrics Are Weapons)

Any time you see a metric-based conclusion, pause and ask: **"What's the definition?"**

Steps:
1. Verify definition consistency across sources (especially when benchmarking)
2. If the metric is inherently fuzzy, force an operational definition (e.g., "inactive for X days")
3. Treat media-reported competitor metrics as high-bias until proven otherwise

**Benchmarking without aligned definitions is fake precision.** Winning the definition debate is a prerequisite to winning the benchmarking argument.

---

## Market Research Critique (Sample × Attribute × Importance)

Before accepting research conclusions, critique along three dimensions:

1. **Sample apples-to-apples** — who was surveyed, and is the comparison group truly comparable?
2. **Right audience** — are you studying loyal customers, churned customers, or never-customers? The answer changes completely.
3. **Importance weighting** — a big performance gap is meaningless without attribute importance scores. Always force the matrix: **Satisfaction Gap × Importance**
4. **Attribute completeness** — did the survey ask about the right attributes? Missing a critical attribute (e.g., "open times" at a gym) renders findings misleading

**Rule:** If the business question is "why aren't people choosing us," surveying only loyal customers is a category error.

---

## Validate Third-Party Market Numbers

When inheriting a market-size chart or external data, ask:

1. **Source credibility** — who produced it?
2. **Model assumptions** — how did they compute it?
3. **Definition scope** — what's included/excluded? ("Nutraceutical" can mean "healthy foods broadly" or "science-based functional products" — a 10× difference)
4. **Incentives** — why might they inflate or deflate? (Newsletter/journal with subscription revenue has incentive to hype the category)

If you cannot explain the underlying model, do not quote the number. "Easy cut-and-paste charts" create a fake fact base.

---

## Synthesis: 2–3 Executable Conclusions from Many Data Points

Assume data is correct. Then:

1. Extract **patterns**, not scattered observations
2. Build a 2–3 sentence story:
   - What is the structural pattern?
   - What is the implication now?
   - What is the implication next (trajectory)?
3. Allow **disciplined "leaps of faith"** to keep the story simple — while staying grounded enough to defend

**Convert index gaps into dollars** to apply 80/20 correctly. A 15% gap in an index means nothing without knowing the dollar base.

**Storytelling for persuasion:** Combine two datasets when the primary data alone seems comfortable (e.g., "we're leading" + "customer satisfaction is uniformly poor across everyone → ticking time bomb when competitors improve").

---

## Research Deck Discipline (Don't Drown the Client)

1. Tie every slide to the core client question
2. If a finding is interesting but not central → appendix or separate deck
3. Protect against "We already knew that" reactions:
   - **Pre-test conclusions informally** with friendly stakeholders before the big meeting
   - **Pre-belief capture:** collect internal estimates first, show dispersion, then reveal data truth — demonstrates value

More slides reduces perceived value. Sharp relevance increases it.

---

## Robustness Toolkit

**Data level:**
- Use diverse sources; use proxies when direct data is missing
- Never take client/third-party inputs at face value — assess bias + incentives
- Before commissioning research, get stakeholder buy-in on design (sample + attributes)
- Manage the **trust bank** when clients are slow to provide data: ask in smaller bites, deliver visible early value, show why data matters with back-of-envelope sensitivity scenarios

**Analysis level:**
- Apply healthy skepticism to inherited data: definition, methodology, assumptions, comparability
- Apply 80/20 to analysis selection — do the few analyses that matter
- Use top-down vs bottom-up cross-checks

**Conclusion level:**
- Ask: "Could the same data support a different conclusion?" If yes, logic is cracked
- Bolster conclusions via sensitivity analysis + triangulation + co-creation
- Keep models simple and explainable; avoid black boxes
- Continuously move up/down: data ⇄ analysis ⇄ conclusion ⇄ client problem

**Precision rule:** Round numbers to avoid fake precision. In fuzzy contexts, use ranges and round numbers — don't report 1.352B when the underlying model has ±40% uncertainty.
